\documentclass[12pt]{amsart}
\usepackage{graphicx}
\usepackage{palatino,eulervm}
%\usepackage{concrete}

\newcommand{\lik}{\ensuremath{\mathcal{L}}}
\newcommand{\gimli}{\texttt{gimli}}
\newcommand{\ie}{\textit{i.e.}}

\begin{document}
\title{Multiscale segmentation of methylation data}
\author{Emanuele Raineri}
\date{\today}
\maketitle

\begin{abstract}
%Since it is possible to decorate the human genome with tracks of numerical
%values produced by methods for measuring all sorts of physico-chemical interactions 
%(chromatin modification,
%Hi-C, whole genome bisulfite sequencing, and more), 
%it is useful to implement software 
%which can quickly summarize the data and extract patterns to help 
%automating at least  part of the biological analysis. 
In this paper, I show one efficient way of computing 
segmentations of methylation values determined by 
whole genome bisulfite sequencing
A segmentation joins adjacent position
which have similar properties, while the boundary between segments 
indicates more or less abrupt changes
in the signal which might relate to biological mechanisms. 
One salient aspects which must be considered when it comes to 
analyzing methylation data is that this epigenetic mark seems 
to have effect at various genomic scales 
(ranging from hundred of bases to megabases), hence 
one nees a method that works at many scales. 
The probabilistic approach I introduce here 
coupled with a known algorithm previously
used for copy number detection
can be used to calculate automatic segmentations at different scales.
I also show how this 
kind of statistical modeling is expedient for 
visualization, comparison across samples,
and demarcation of unusual regions in 
order to direct further analysis (i.e. regions with stark variations in the signal).
\end{abstract}

\section{Introduction}

Whole Genome Bisulfite sequencing (WGBS) allows measurements of methylation 
status with single base resolution across an entire genome. The outcome
of a WGBS experiment is a count of so called {\it non converted} and {\it converted} 
reads at each 
sequenced position : the methylation level, $\theta$, is defined
as the ration of the non converted reads over the total of converted and non converted reads.
Methylation has a strong local correlation,
in the sense that the level at one position predicts 
very well the observations nearby; this suggests that it should be possible
to build compressed representations of the data in which contiguous loci 
are collected into segments.
This local correlation is depicted in figure~\ref{fig_corr},
and it has been observed indipendently many times (for example
see \cite{bsmooth} and references therein).

\begin{center}
\begin{figure}\label{fig_corr}
\includegraphics[width=7cm,angle=270]{out.correla.eps}
\caption{This plot is produced by extracting $1$ million random pairs of CpG loci;
storing the pairs in $20$ different bins  according to the distance between the members
of the pair (from $0-100$ to $1900-2000$); and computing the correlation in each bin,
using the pairs contained in it.}
\end{figure}
\end{center}
 
Changes in methylation can be observed both at the level of a single promoter
region (hundreds of bases, \cite{methylseekr}) and when looking at stretches
of millions of bases (\cite{largeblocks} ).
This brings about contrasting requirements when analyzing this kind of data: large scale (slow) changes are 
significant and must be looked at, but a rough
running average with a window of many megabases might destroy interesting
local detail. Here I present  a software (\gimli{}) which computes a multiscale segmentation 
of DNA methylation 
data acquired through WGBS and helps making sense of the data and comparing
it across samples.

A segmentation is a statistical model of a dataset  which takes into account its spatial distribution,
\ie{} adjacent points with similar properties are grouped together.
In general, when building a statistical model one tries to find a reasonable 
trade off between goodness of fit and complexity.
In our case these two aspects have a very simple interpretation : they both typically 
increase with the number of segments $N_S$ used to describe the 
methylation data.  
At one extreme, representing the complete dataset with one big segment would 
give a very simple model with, but likely a very bad fit;  
at the other using many segments of length $1$ would normally produce
a well fitting but very complex summary of the data.
The complexity of the model is linked to the typical scale of the segments used
in the sense that if the expected size of a block is very large one needs few of them to 
cover the genomic region under consideration.

\section{The algorithm}
The basic idea underpinning this method is to find a segmentation
by optimizing  a function $\psi$ which depends on the coordinates
of each block as well as on the spatial distribution of the CpGs.
Since the optimization is carried out
in a greedy fashion there's no guarantee
that it finds a global maximum, rather a good enough maximum for practical
purposes.
\subsection{Blocks and their likelihood}
The  input to \gimli{} is a set of $N$ methylated positions 
which do not specifically need to be CpGs.
In the rest of the paper, though, I will refer to CpGs. Moreover 
I will call {\em block} or {\em segment} 
both a single CpG or a set of contiguous CpGs. 
Note that when I say contiguous I mean that they are one
after the other along the genome, not necessarily that they are
close in genomic coordinates. CpGs are 
indexed by $j=1,\dots,N$ : for example, for human chromosome $1$ 
one has $N = 2284470$ in the human reference genome version $37$.
This is the maximum number of loci that a WGBS experiment can capture: 
in practice, even at a decent read depth,
some of them do not appear in the sequencing result. 
The number of nonconverted (respectively, converted) reads mapping at 
position $j$ is $\overline{n}_j$ (respectively, $n_j$). 

The output of \gimli{} is a set of blocks 
covering the initial positions 
such that each block contains CpGs which are similarly
methylated. The strength with which
one imposes the similarity constraint
is controlled by a coarseness parameter $\lambda$.
Internally the program  mantains a list of the current  
blocks (indexed by $i=1,\dots,N_S$), 
which at the beginning 
coincides with the loci given in input. 
( i.e.  $N_S=N$ and each block has length $1$). 
To each segment $i$ there is associated a $\hat{\theta}_i$ which
is the maximum likelihood estimation of the methylation given the counts
at each position in the segment. 
For example, if segment $i$ includes the CpGs from $j_1$ to $j_2$
the the maximum likelihood estimation for $\hat{\theta}_i$ is:

\[
\hat{\theta}_i=\frac{\overline{n}}{n + \overline{n}}
\]

where

\[
\overline{n}=\sum_{j=j_1}^{j_2} \overline{n}_j
\]

and

\[
n=\sum_{j=j_1}^{j_2} n_j
\]

The likelihood of such a block is defined as follows:

\begin{equation}
\lik_i(\hat{\theta}_i)=\sum_{j=j_1}^{j_2} 
\log {\overline{n}_j+n_j \choose \overline{n}_j} +
	{\overline{n}_j}\log\hat{\theta}_i+
	n_j\log(1-\hat{\theta}_i)
\end{equation}
\label{loglik}

I will use a boldface
$\pmb{\theta}_i$ to refer to the vector of the methylation values of all the loci belonging to 
block $i$.  The likelihood is used as a fundamental component in the function $\psi$, defined below,
which is to be maximized.

\subsection{merging blocks}

The way the program works is by 
scanning repeatedly the list of blocks and evaluate whether any pair 
of adjacent ones can be conveniently merged into one.  
This is decided by checking whether the loss in 
likelihood consequent to the merging is compensated 
(or more than compensated)
by the reduction in complexity achieved by it.

After a merging, let $\hat{\theta}_{i,i+1}$ be the mle estimation of methylation of 
a segment which includes
all the loci in segments $i$ and $i+1$.
The corresponding change in likelihood is 
Notice that it can be  
\[\Delta  \lik_{i,i+1} = \lik_i(\hat{\theta}_{i,i+1})+
	\lik_{i+1}(\hat{\theta}_{i,i+1}) 
	-\lik_i(\hat{\theta}_{i})
	-\lik_{i+1}(\hat{\theta}_{i+1}) \] 

Note that $\Delta  \lik_{i,i+1} \leq 0$: this is counterbalanced, though, by the fact the the number 
of segments decreases by $1$. To take into account this decrease in 
complexity one can consider the following expression:

\[\Delta  \lik_{i,i+1}  + \lambda\]

Furthermore, one has to take into account that methylated sites are irregularly 
spaced along the genome and the variability 
introduced by the sequencing process usually increases the distance 
between adjacent sites.
To avoid building segments which span long regions where no CpGs exist, one 
can multiply the difference in likelihoods by a sigmoidal penalty term of the
form \[\rho_{i,i+1}=\frac{D1}{1+\exp(-D_{i,i+1}/D0)}-\frac{D_1}{2}\] (see fig\ref{figrho}) 
where $D_{i,i+1}$ is the distance 
between the the right end of segment $i$ and the left end of segment $i+1$ and 
$D_0,D_1$ are constants which  I set heuristically to  $D_0=10000,D_1=100$. 

\begin{figure}\label{figrho}
\includegraphics[width=7cm,angle=270]{figrho.eps}
\caption{Empirical penalty factor $\rho$}
\end{figure}

Hence the final expression for $\psi_{i,i+1}$ is:

\begin{equation}
\psi_{i,i+1} =  \Delta  \lik_{i,i+1}  - \rho_{i,i+1}  +\lambda
\end{equation}

Once it has computed 
$\psi_{i,i+1}$ for all $i$, the algorithm selects the maximum 
one : if this is positive, it conjoins the corresponding segments into one.

The software repeats the scanning and merging until $\psi_{i,i+1} \leq 0$ 
which means that it is not possible to 
merge any more pair; after that it can either increment $\lambda$ and try again, 
or give up, print the segmentation and exit, depending on the command line parameters.

The above procedure is similar to the one
described in the context of copy number variation detection in \cite{vega} 
( which, in turn, us an adaptation 
of a 2 dimensional image analysis algorithm due to Mumford and Shah, 
\cite{mumfordshah}) except that the probabilistic method  described
here is more suitable for analyzing methylation data. This is because it 
takes into account that the
uncertainty with which we know $\hat{\theta}_i$ is higher when the read depth 
is low and vice versa.
This has an effect on the decision of merging adjacent blocks. I can show that 
this is the case by simulating
a the counts (at different read depths) for a pair of loci with 
methylations respectively 
$\theta_1=0.8$ and $\theta_2=0.45$,
and computing the corresponding $\Delta  \lik$.  
As figure \ref{fig_delta_cov} shows the difference in likelihood is higher 
at higher read depth, when the methylation values are more precisely known.

\section{Results}

Reassuringly, the segments created by \gimli{} to have lower methylation
variability than random segments of the same length extracted from the 
input dataset. 
This is shown in \ref{fig_variance}.

\begin{figure}\label{fig_variance}
\includegraphics[width=7cm,angle=270]{fig_variance.eps}
\caption{Variance across segments of $15$ contiguous CpGs when chosen at 
random or 
computed with \gimli{}. Methylation values from }
\end{figure}

\begin{figure}\label{fig_delta_cov}
\includegraphics[width=7cm,angle=270]{fig_delta_cov.eps}
\caption{$\Delta  \lik$ at fixed methylation difference depends on read depth.}
\end{figure}

I can now refine quantitatively the assertion that the complexity of the model
is linked to the typical lengths of the segments : as figure \ref{fig_boxplot_size} 
shows the median length of the blocks increases with $\lambda$. This is 
accompanied by a decrease in likelihood of the segments (\ref{fig_boxplot_lik})

\begin{figure}\label{fig_boxplot_size}
\includegraphics[width=7cm,angle=270]{fig_boxplot_size.eps}
\caption{Median length of blocks is higher at higher $\lambda$s.}
\end{figure}

\begin{figure}\label{fig_boxplot_lik}
\includegraphics[width=7cm,angle=270]{fig_boxplot_lik.eps}
\caption{Likelihood of blocks diminishes with $\lambda$.}
\end{figure}

While one can verify that the numbers computed by the algorithm are in 
accordance with the description
given in this paper, the purpose of \gimli{} is to help 
detect automatically interesting biological patterns in methylation data, 
and this can't be tested in any 
precise sense. To show that \gimli{} is a useful tool
though, I will give $4$ examples of analyses which can be produced very quickly
using it. 

\subsection{PMD}

First I will show how segmentation can help visualizing methylation data by choosing
a stretch where many possible values of $\theta$ are present, a partially
methylated domain (taken from \cite{pmd}) with coordinated 
\texttt{chr10:112000000-122000000}. Here the data comes from the Blueprint sample \texttt{G199}. 

The figure below corresponds to segmenting the same genomic region (chr10, from 1.1885e8,1.1893e8) at 
three different levels of coarseness encoded by the parameter $\lambda$ : 
these three levels also broadly correspond to three different inherent spatial scales for the segmentation,and allow one to 
see by eye interesting substructures. 

\begin{figure}\label{fig_g199_pmd_chr10.eps}
\includegraphics[width=7cm,angle=270]{fig_g199_pmd_chr10.eps}
\caption{Partially methylated domain in chr10 segmented with $3$ incleasing $\lambda$s.}
\end{figure}


\subsection{Jumps}

First I'll show how the boundary of the segments, (its  breakpoints)
can correspond to landmarks in the genome. To this purpose, I considered the 
$4$ Blueprint monocytes \verb=C000S5A1bs=,\verb=C0010KA2bs=,\verb=C001UYA3bs= and \verb=C004SQ51= 
I computed their segmentation for $\lambda=10$ and looked for all the triples of consecutive segments
with corresponding vectors methylation values $\pmb{\theta}_{1},\pmb{\theta}_{2},\pmb{\theta}_{3}$
where 
\begin{itemize}
\item{} $\min \pmb{\theta}_1 \geq \max \pmb{\theta}_2 + 0.25$
\item{} $\min \pmb{\theta}_{3} \geq \max \pmb{\theta}_2 + 0.25$
\item{} all the segments contain at least $5$ CpGs.
\item{} they must appear in all the 4 samples
\end{itemize}

These conditions correspond to regions where the methylation goes from 
high to low to high again,
in jumps of at least $0.25$.
See for example figure \ref{fig_jumps} 

In this way I obtain $143$ such triplets. Looking at their central segmentes
(those in a comparatively low methylation states) I find that their median length is
$252$bp (minimum length $39$, maximum $252$). When I intersect this central segments with the Blueprint
annotation, I find than $94$ overlap a known transcript, suggesting that regions with stark 
changes in methylation
might sistematically correspond to regulated parts of the genome.

\begin{figure}\label{fig_jumps}
\includegraphics[width=7cm,angle=270]{fig_jumps.eps}
\caption{An example of high-low-high pattern found in chromosome $1$ by \gimli{}}
\end{figure}

\subsection{Differentially methylated regions}

\gimli{} can be used to catch differentially methylated regions across pair of samples
or pair of groups of samples.
As an example, in what follows I use it to find DMRs between monocytes and \texttt{M0} macrophages 
samples in Blueprint.  I examined the following samples:

\begin{center}
\begin{tabular}{c|c}
monocytes & M0  \\
\hline \\
\texttt{C000S5A1bs} & \texttt{C005VG51} \\
\texttt{C0010KA2bs} & \texttt{S001S751} \\
\texttt{C004SQ51} & \texttt{S0039051} \\
\texttt{C005PS51} & \texttt{S00BHQ51}\\
\texttt{S000RD54} & \texttt{S00DVR51} \\
\texttt{S007G756} &  \\
\hline
\end{tabular}
\end{center}

%monocytes: \texttt{C000S5A1bs},\texttt{C0010KA2bs},\texttt{C004SQ51},\texttt{C005PS51},\texttt{S000RD54},\texttt{S007G756}
%M0 macrophages: \texttt{C005VG51},\texttt{S001S751},\texttt{S0039051},\texttt{S00BHQ51},\texttt{S00DVR51}


As an example, we consider on one side the 
first we consider all the positions where at least one of the samples has been measured
(WGBS loses positions dependeing on the fluctuations in read depth typical of high throughput sequencing: 
each sample is usually measured in a slightly different set of positions).

Now, \gimli{} only accepts a single set of counts of nonconverted and converted reads per position
hence in order to run it on a group of samples I need to do some pre-processing:  I 
summarize the counts by first transforming them in an array of estimated methylation
values and then finding (by moment matching) a beta distribution which fits them.

The parameters of the Beta distribution can be interpreted as counts of 
a non converted and converted reads (see for example \cite{methyldiff} for a full explanation) 
of a single sample experiment giving the same methylation mean 
and variance as a maximum likelihood estimation from the available samples.

To find the differentially methylated regions, I start by intersecting
( with \texttt{bedtools}\cite{bedtools} ) the segmentation computed for the first group
with the segmentation of the second : whenever two segments intersect I look
for the ratio between the difference in average methylation in the common area 
and the square root of the sum of the variances of the two samples divided by their respective
sizes; one can then run a t-test.

To tue purpose of illustrating how it might work I plot here a DMR 
\texttt{chr1:27994303-27994356}.

Notice that obviously one can compare segmentations corresponding to one sample
each (so that the pre processing above is not needed). This could be useful, for example, 
when contrasting pair of technical replicates agains each other.

\section{Implementation}

\gimli{} is written in \texttt{C}, and the source
dode is available from the github page of the author. 
On a Pentium(R) Dual-Core  CPU (\texttt{E5400@2.70GHz})
with $2048$ KB of cache it takes $\approx 840s$ to process the 
methylation levels of the sample \texttt{C001UYA3bs} ($23815993$ CpGs) 
with $4$ different $\lambda$s ($1,10,100$ and $1000$).
This includes the time for decompressing the input file and selecting some of its columns
with \texttt{awk}.


\bibliography{gimli_paper}
\bibliographystyle{plain}
\end{document}
